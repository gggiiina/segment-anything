import os
import json
import cv2
import numpy as np
from PIL import Image
from collections import defaultdict
from segment_anything import SamPredictor, sam_model_registry
import torch
import clip

def segment_and_save(
    sam_data_path="sam_data.json",
    image_dir="test_image",
    output_dir="seg_pic",
    sam_checkpoint="sam_vit_h_4b8939.pth"
):
    """ä½¿ç”¨ SAM åˆ†å‰²æ‰€æœ‰ç‰©ä»¶ä¸¦å„²å­˜åˆ‡åœ–åˆ° seg_pic"""
    os.makedirs(output_dir, exist_ok=True)

    with open(sam_data_path, "r") as f:
        sam_data = json.load(f)

    grouped_data = defaultdict(list)
    for item in sam_data:
        grouped_data[item["filename"]].append(item)

    print(f"ğŸ§  è¼‰å…¥ SAM æ¨¡å‹ä¸­...")
    sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
    predictor = SamPredictor(sam)
    print(f"âœ… SAM æ¨¡å‹è¼‰å…¥å®Œæˆ")

    global_idx = 0

    for image_idx, (filename, detections) in enumerate(grouped_data.items(), start=1):
        image_path = os.path.join(image_dir, filename)
        image = cv2.imread(image_path)

        if image is None:
            print(f"âŒ åœ–ç‰‡è®€å–å¤±æ•—ï¼š{image_path}ï¼Œè·³é")
            continue

        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        predictor.set_image(image_rgb)

        for i, item in enumerate(detections):
            label = item["label"]
            box = np.array(item["box"])
            box_input = box[None, :]

            masks, _, _ = predictor.predict(box=box_input, multimask_output=False)
            mask = masks[0]

            mask_3c = np.stack([mask] * 3, axis=-1)
            cutout = image_rgb * mask_3c
            x0, y0, x1, y1 = box.astype(int)
            cropped = cutout[y0:y1, x0:x1]

            if cropped.shape[0] < 10 or cropped.shape[1] < 10:
                print("âš ï¸ å€å¡Šå¤ªå°ï¼Œç•¥é")
                continue

            clean_label = label.replace("/", "-").replace("\\", "-")
            out_name = f"{global_idx}_{clean_label}_0.png"
            out_path = os.path.join(output_dir, out_name)

            success = cv2.imwrite(out_path, cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))
            if success:
                print(f"âœ… å·²å„²å­˜ï¼š{out_path}")
                global_idx += 1
            else:
                print(f"âŒ å„²å­˜å¤±æ•—ï¼š{out_path}")


def extract_clip_features(seg_pic_dir="seg_pic", output_json="clip_features.json"):
    """é‡å° seg_pic è£¡çš„æ‰€æœ‰åœ–åƒåš CLIP ç‰¹å¾µæŠ½å–ï¼Œä¸¦å„²å­˜æˆ JSON"""
    print("ğŸ§  è¼‰å…¥ CLIP æ¨¡å‹ä¸­...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)
    print(f"âœ… CLIP æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆä½¿ç”¨è£ç½®ï¼š{device}ï¼‰")

    features_dict = {}

    files = sorted([
        f for f in os.listdir(seg_pic_dir)
        if f.lower().endswith((".png", ".jpg", ".jpeg"))
    ])

    for idx, fname in enumerate(files, start=1):
        fpath = os.path.join(seg_pic_dir, fname)
        pil_img = Image.open(fpath).convert("RGB")

        image_tensor = preprocess(pil_img).unsqueeze(0).to(device)
        with torch.no_grad():
            feature = model.encode_image(image_tensor).cpu().numpy()[0]

        key = os.path.splitext(fname)[0]
        features_dict[key] = {
            "filename": fname,
            "feature": feature.tolist()
        }

        print(f"ğŸ” [{idx}/{len(files)}] ç‰¹å¾µå·²æŠ½å–ï¼š{key}")

    with open(output_json, "w") as f:
        json.dump(features_dict, f)

    print(f"\nğŸ‰ å…¨éƒ¨ç‰¹å¾µå·²æŠ½å–ï¼Œå„²å­˜è‡³ï¼š{output_json}")


# ğŸ ç¯„ä¾‹åŸ·è¡Œæµç¨‹ï¼ˆå¯æ”¾ main å‡½å¼ä¸­ï¼‰
if __name__ == "__main__":
    segment_and_save()
    extract_clip_features()
